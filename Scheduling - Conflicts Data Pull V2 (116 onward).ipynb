{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Committee Scheduling Conflicts\n",
    "\n",
    "This scripts analyzes the House Committee Respository to collect the number of scheduling conflicts in each member's schedule.\n",
    "\n",
    "If the same committee has two hearings in the same hour (for example, a committee hearing and a markup), only one of hearing is counted in the analysis of conflicts.\n",
    "\n",
    "#### NOTE TO SELF - SEARCH XXX FOR INPUT CHANGES BEFORE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "from lxml import html    \n",
    "from datetime import datetime, timedelta\n",
    "import re    \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pulls hearings for one day\n",
    "def hearingpull(date):\n",
    "\n",
    "    #formatting url\n",
    "    day_url = \"https://docs.house.gov/Committee/Calendar/ByDay.aspx?DayID=\" + date.strftime(\"%m%d%Y\")\n",
    "\n",
    "    #import webpage and create tree\n",
    "    webpage = requests.get(day_url)\n",
    "    tree = html.fromstring(webpage.content)\n",
    "\n",
    "    #importing and formatting hearing titles\n",
    "    hearing_titles = tree.xpath(\"//table//a[@title]//text()[normalize-space()]\")\n",
    "    hearing_titles = [sub.replace('\\r\\n', '') for sub in hearing_titles] \n",
    "    hearing_titles = [sub.strip() for sub in hearing_titles]\n",
    "    hearing_titles = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in hearing_titles]\n",
    "    hearing_titles = [str(sub).replace(\"???\",\" \") for sub in hearing_titles]\n",
    "    hearing_titles = [str(sub).replace(\"'\",\"\") for sub in hearing_titles]\n",
    "    hearing_titles = [str(sub).replace('\"',\"\") for sub in hearing_titles]\n",
    "\n",
    "    #import and format committee titles\n",
    "    committee_titles = tree.xpath(\"//table//span[@title]//text()[normalize-space()]\")\n",
    "    committee_titles = [sub.replace('\\r\\n', '') for sub in committee_titles] \n",
    "    committee_titles = [sub.strip() for sub in committee_titles]\n",
    "    committee_titles = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in committee_titles]\n",
    "    committee_titles = [str(sub).replace(\"???\",\" \") for sub in committee_titles]\n",
    "    committee_titles = [str(sub).replace(\"'\",\"\") for sub in committee_titles]\n",
    "    committee_titles = [str(sub).replace('\"',\"\") for sub in committee_titles]\n",
    "\n",
    "    #import and format links\n",
    "    link_extension = tree.xpath(\"//table//a//@href\")\n",
    "    link_extension = [\"https://docs.house.gov/Committee/Calendar/\"+ex for ex in link_extension]\n",
    "\n",
    "    #import times and dates\n",
    "    times = list()\n",
    "    dates = list()\n",
    "\n",
    "    for hearing in range(len(link_extension)):\n",
    "        hearing_link = requests.get(link_extension[hearing])\n",
    "        hearing_tree = html.fromstring(hearing_link.content)\n",
    "        try:\n",
    "            datestring = hearing_tree.xpath(\"//div[@class='meeting-date']//p/text()[normalize-space()]\")\n",
    "            #get dates\n",
    "            date = datestring[0][0:datestring[0].find(\"(\")-1]\n",
    "            date = date.replace('\\r\\n', '')\n",
    "            date = date.strip()\n",
    "            #get times\n",
    "            time = datestring[0][datestring[0].find(\"(\"):len(datestring[0])]\n",
    "            time = time.replace('\\r\\n', '')\n",
    "            time = time.replace('(', '')\n",
    "            time = time.replace(')', '')\n",
    "            if time.find(\"-\") > 0:\n",
    "                time = time[0:time.find(\"-\")]\n",
    "            time = time.strip()\n",
    "        except:\n",
    "            date = tree.xpath(\"//div[@id='body']//span[@id='LabelPageTitle']//text()[normalize-space()]\")[0]\n",
    "            times = tree.xpath(\"//div//table[@class='table table-bordered']//tr//td[2]//span[@class='text-small']//text()[normalize-space()]\")\n",
    "            times = [sub.replace('\\r\\n', '') for sub in times] \n",
    "            times = [sub.strip() for sub in times]\n",
    "            times = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in times]\n",
    "            times = [str(sub).replace(\"???\",\" \") for sub in times]\n",
    "            times = [str(sub).replace(\"'\",\"\") for sub in times]\n",
    "            times = [str(sub).replace('\"',\"\") for sub in times]\n",
    "            time = times[hearing]\n",
    "        #append\n",
    "        dates += [date]\n",
    "        times += [time]\n",
    "\n",
    "    #zipping into single dataframe\n",
    "    day_results = pd.DataFrame(zip(dates,committee_titles,hearing_titles,times,link_extension),columns=[\"Date\",\"Committee\",\"Hearing Title\",\"Time\",\"Link\"])\n",
    "\n",
    "    return(day_results)\n",
    "\n",
    "#this pulls hearings for a date range\n",
    "def gethearingrange(datestart,dateend):\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"Date\",\"Committee\",\"Hearing Title\",\"Time\",\"Link\"])\n",
    "\n",
    "    datestart = datetime.strptime(datestart,\"%m/%d/%Y\").date()\n",
    "    dateend = datetime.strptime(dateend,\"%m/%d/%Y\").date()\n",
    "    \n",
    "    #if dateend is in the future, make dateend today\n",
    "    if dateend > datetime.now().date():\n",
    "        dateend = datetime.now().date()\n",
    "\n",
    "    while datestart <= dateend:\n",
    "        # results = results.append(hearingpull(datestart)) #append dep\n",
    "        results = pd.concat([results, hearingpull(datestart)], ignore_index=True)\n",
    "        datestart += timedelta(days=1)\n",
    "    \n",
    "    #remove misc. spaces from committee column\n",
    "    results[\"Committee\"] = [re.sub(' +', ' ',com) for com in results[\"Committee\"]]\n",
    "    \n",
    "    results[\"Time\"] = [time.replace(\"local time\",\"\") for time in results[\"Time\"]]\n",
    "    results[\"Time\"] = [time.strip() for time in results[\"Time\"]]\n",
    "    \n",
    "    results = results.drop_duplicates(subset=[\"Link\"])\n",
    "    \n",
    "    results[\"Hour\"] = [datetime.strptime(time,\"%I:%M %p\").hour for time in results[\"Time\"]]\n",
    "    results.drop_duplicates(subset=[\"Date\",\"Committee\",\"Hour\"],inplace=True)\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE: GATHER COMMITTEE ASSIGNMENTS \n",
    "This part takes forever if you're doing prior congresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getassignments(congress):\n",
    "\n",
    "    ##uses web archive to get committee assignments for each congress (archive not needed for current congress)\n",
    "    Com_Dict = {118:\"\",\n",
    "                117:\"https://web.archive.org/web/20221019171459\", #XXX\n",
    "                116:\"https://web.archive.org/web/20201026031027/\",\n",
    "                115:\"https://web.archive.org/web/20181026031027/\"}\n",
    "\n",
    "\n",
    "    member_data = pd.DataFrame()\n",
    "    clerk = Com_Dict.get(congress)+\"https://clerk.house.gov/committees\"\n",
    "\n",
    "    webpage = requests.get(clerk)\n",
    "    tree = html.fromstring(webpage.content)\n",
    "\n",
    "\n",
    "    com_titles = tree.xpath(\"//div[@class='col-sm-11 col-xs-10 library-committeePanel-heading']//a//text()\")\n",
    "    com_links = tree.xpath(\"//div[@class='col-sm-11 col-xs-10 library-committeePanel-heading']//a//@href\")\n",
    "\n",
    "    ComLink_Dict = {118:\"http://clerk.house.gov\",#XXX\n",
    "                    117:\"https://web.archive.org/\",\n",
    "                    116:\"https://web.archive.org/\",\n",
    "                    115:\"https://web.archive.org/\"}\n",
    "\n",
    "    com_links = [ComLink_Dict.get(congress)+end for end in com_links]\n",
    "    com_links = com_links + [\"http://clerk.house.gov/committees/VC00\"]\n",
    "    com_codes = [title[len(title)-4:len(title)] for title in com_links]\n",
    "\n",
    "\n",
    "    for com in range(len(com_links)):\n",
    "\n",
    "        singlecom = requests.get(com_links[com])\n",
    "        tree = html.fromstring(singlecom.content)\n",
    "\n",
    "        members = tree.xpath(\"//ul[@id='majority-members' or @id='minority-members']//li/a/span/text()\")\n",
    "        members = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in members]\n",
    "        members = [str(member).replace(\"??\",\"e\") for member in members]\n",
    "        members = [str(member).replace(\"?\",\"e\") for member in members]\n",
    "\n",
    "        # member_data = member_data.append(pd.DataFrame(members,columns=[com_codes[com]]).transpose()) #APPEND DEPRECATED\n",
    "        member_data = pd.concat([member_data, pd.DataFrame(members,columns=[com_codes[com]]).transpose()])\n",
    "\n",
    "        subcom_links = tree.xpath(\"//section[@class='subcommittees']//ul[@class='library-list_ul']//li//a/@href\")\n",
    "        subcom_links = [ComLink_Dict.get(congress)+end for end in subcom_links]\n",
    "        subcom_links = [link.replace(\"///\",\"/\") for link in subcom_links]\n",
    "\n",
    "\n",
    "        subcom_codes = [title[len(title)-4:len(title)] for title in subcom_links]\n",
    "\n",
    "        for subcom in range(len(subcom_links)):\n",
    "            single_subcom = requests.get(subcom_links[subcom])\n",
    "            tree = html.fromstring(single_subcom.content)\n",
    "            members = tree.xpath(\"//ul[@id='majority-members' or @id='minority-members']//li/a/span/text()\")\n",
    "            members = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in members]\n",
    "            members = [str(member).replace(\"??\",\"e\") for member in members]\n",
    "            members = [str(member).replace(\"?\",\"e\") for member in members]\n",
    "            # member_data = member_data.append(pd.DataFrame(members,columns=[subcom_codes[subcom]]).transpose()) #APPEND DEPRECATED\n",
    "            member_data = pd.concat([member_data, pd.DataFrame(members,columns=[subcom_codes[subcom]]).transpose()])\n",
    "\n",
    "    return(member_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO: GET COMMITTEE CODES FOR HEARINGS, GATHER HEARING DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethearingdata(congress,member_data):\n",
    "\n",
    "    dates_start = {118:\"01/03/2023\",\n",
    "                   117:\"01/03/2021\",\n",
    "                   116:\"01/03/2019\",\n",
    "                   115:\"01/03/2017\"}\n",
    "    \n",
    "    dates_end = {118:\"01/02/2025\",#XXX\n",
    "                 117:\"01/02/2023\",\n",
    "                 116:\"01/02/2021\",\n",
    "                 115:\"01/02/2019\"}\n",
    "    \n",
    "    hearing_data = gethearingrange(dates_start.get(congress),dates_end.get(congress))\n",
    "\n",
    "    #import replacement (comcode) files\n",
    "\n",
    "    replacement = pd.read_csv(str(\"https://raw.githubusercontent.com/rachelorey/Scheduling-Conflicts-in-Congress/master/replacement\"+str(congress)+\".csv\"))\n",
    "\n",
    "    # #drop all comcodes without member assignments from clerk.gov\n",
    "    codes_to_drop = [value for value in replacement[\"Code\"].unique() if value not in member_data.index.unique()]\n",
    "    replacement = replacement[~replacement[\"Code\"].isin(codes_to_drop)]\n",
    "\n",
    "    #convert committees to lowercase for merging\n",
    "    hearing_data[\"committee-low\"] = hearing_data[\"Committee\"].str.lower()\n",
    "    replacement[\"committee-low\"] = replacement[\"Committee\"].str.lower()\n",
    "    \n",
    "    #drop original column in replacement df\n",
    "    replacement.drop([\"Committee\"],axis=1,inplace=True)\n",
    "    \n",
    "    #merge codes and names\n",
    "    hearing_data = pd.merge(hearing_data,replacement,on=\"committee-low\",how=\"left\")\n",
    "    \n",
    "    #drop lowercase column\n",
    "    hearing_data.drop([\"committee-low\"],axis=1,inplace=True)\n",
    "    \n",
    "    return(hearing_data)\n",
    "\n",
    "def testmatches(hearing_data):\n",
    "    import pandas as pd\n",
    "    match = pd.DataFrame(hearing_data[hearing_data[\"Code\"].isna()][\"Committee\"].unique())\n",
    "    return(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART THREE: LOOK FOR SCHEDULING CONFLICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconflicts(member_data,hearing_data):\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"MC\",\"Hearing 1 Code\",\"Hearing 1 Link\",\"Hearing 2 Code\",\"Hearing 2 Link\",\"Date\"])\n",
    "    unique_dates = hearing_data[\"Date\"].unique()\n",
    "\n",
    "    for unique_day in unique_dates:\n",
    "\n",
    "        #get dataframe of all hearings in selected day\n",
    "        day = hearing_data[hearing_data[\"Date\"]==unique_day]\n",
    "\n",
    "        #make sure there are at least two different committees meeting today\n",
    "        if len(day[\"Code\"].unique()) >= 2:\n",
    "\n",
    "            #ADD TWO HOURS TO HEARINGS TO CREATE HEARING LENGTH\n",
    "\n",
    "            day[\"Time\"] = [datetime.strptime(time,\"%H:%M %p\") for time in day[\"Time\"]]\n",
    "            counts = day[\"Code\"].value_counts()\n",
    "\n",
    "            Time_2 = list()\n",
    "\n",
    "\n",
    "            ### XXX Note to self - look into why i set it up this way before rather than just adding two hours across the board\n",
    "            for index, row in day.iterrows():\n",
    "                if counts.loc[row[\"Code\"]] == 1:\n",
    "                    Time_2.append(row[\"Time\"]+timedelta(hours=2))\n",
    "                elif row[\"Time\"]+timedelta(hours=2) < day[day[\"Code\"]==row[\"Code\"]][\"Time\"].max():\n",
    "                    Time_2.append(row[\"Time\"]+timedelta(hours=2))\n",
    "                elif row[\"Time\"] == day[day[\"Code\"]==row[\"Code\"]][\"Time\"].max():\n",
    "                    Time_2.append(row[\"Time\"]+timedelta(hours=2))\n",
    "                else:\n",
    "                    Time_2.append(day[day[\"Code\"]==row[\"Code\"]][\"Time\"].max())\n",
    "\n",
    "\n",
    "            day[\"Time+2\"] = Time_2\n",
    "\n",
    "            #get overlapping hearings\n",
    "            combos = pd.DataFrame(itertools.combinations(day.index,2),columns = [\"Hearing Code 1\",\"Hearing Code 2\"])\n",
    "\n",
    "            overlap = list()\n",
    "            for combo in range(len(combos)):\n",
    "                hearing1 = combos[\"Hearing Code 1\"][combo]\n",
    "                hearing2 = combos[\"Hearing Code 2\"][combo]\n",
    "                latest_start = max(day[\"Time\"][hearing1],day[\"Time\"][hearing2])\n",
    "                earliest_end = min(day[\"Time+2\"][hearing1],day[\"Time+2\"][hearing2])\n",
    "                if (earliest_end - latest_start) > timedelta(hours=0):\n",
    "                    overlap.append(\"Overlaps\")\n",
    "                else:\n",
    "                    overlap.append(\"No Overlap\")\n",
    "            combos[\"Overlap\"] = overlap\n",
    "            combos = combos[combos[\"Overlap\"]==\"Overlaps\"]\n",
    "            combos.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            #for each combination of committees in one day, determine which committees conflict \n",
    "                #and then get the members that are in both\n",
    "\n",
    "            # change time if it's same committee overlapping\n",
    "            for combo in range(len(combos)):\n",
    "                hearing_1 = day[day.index==combos[\"Hearing Code 1\"][combo]]\n",
    "                hearing_2 = day[day.index==combos[\"Hearing Code 2\"][combo]]\n",
    "\n",
    "                hearing_1.reset_index(drop=True,inplace=True)\n",
    "                hearing_2.reset_index(drop=True,inplace=True)\n",
    "\n",
    "                hearing_1 = hearing_1[\"Code\"][0]\n",
    "                hearing_2 = hearing_2[\"Code\"][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for combo in range(len(combos)):\n",
    "                #get committee code for hearing\n",
    "                hearing_1 = day[day.index==combos[\"Hearing Code 1\"][combo]]\n",
    "                hearing_2 = day[day.index==combos[\"Hearing Code 2\"][combo]]\n",
    "\n",
    "                hearing_1.reset_index(drop=True,inplace=True)\n",
    "                hearing_2.reset_index(drop=True,inplace=True)\n",
    "\n",
    "                hearing_1 = hearing_1[\"Code\"][0]\n",
    "                hearing_2 = hearing_2[\"Code\"][0]\n",
    "\n",
    "                #get members in relevant hearings\n",
    "                try:\n",
    "                    hearing_1_members = member_data[member_data.index == hearing_1].dropna(axis=1).iloc[0,:]\n",
    "                    hearing_2_members = member_data[member_data.index == hearing_2].dropna(axis=1).iloc[0,:]\n",
    "\n",
    "\n",
    "                    #check to make sure it is not the same committee conflicting\n",
    "                    if hearing_1 != hearing_2:\n",
    "                        #get members that are in both conflicting committees\n",
    "                        overlapping_members = [value for value in hearing_1_members if str(value) in str(hearing_2_members)]\n",
    "                        overlapping_members = pd.DataFrame(overlapping_members)\n",
    "                        #if there are overlapping members, add to results\n",
    "                        if len(overlapping_members)>0:\n",
    "                            hearinglist = [[day.loc[combos[\"Hearing Code 1\"][combo]][\"Code\"]]*len(overlapping_members),\n",
    "                                       [day.loc[combos[\"Hearing Code 1\"][combo]][\"Link\"]]*len(overlapping_members),\n",
    "                                       [day.loc[combos[\"Hearing Code 2\"][combo]][\"Code\"]]*len(overlapping_members),\n",
    "                                       [day.loc[combos[\"Hearing Code 2\"][combo]][\"Link\"]]*len(overlapping_members),\n",
    "                                      [day.loc[combos[\"Hearing Code 2\"][combo]][\"Date\"]]*len(overlapping_members)]\n",
    "                            hearinglist = pd.DataFrame(hearinglist).transpose()\n",
    "                            res = pd.merge(overlapping_members,hearinglist,left_index=True,right_index=True)\n",
    "                            res.columns = [\"MC\",\"Hearing 1 Code\",\"Hearing 1 Link\",\"Hearing 2 Code\",\"Hearing 2 Link\",\"Date\"]\n",
    "                            results = pd.concat([results, res])\n",
    "                            #results = results.append(res)  \n",
    "\n",
    "                except:\n",
    "                    print(\"Issue with: \",hearing_1,\" or \",hearing_2)\n",
    "\n",
    "    results.reset_index(inplace=True,drop=True)\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART FOUR: RUN EVERYTHING TO GET RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runeverything(congress):\n",
    "\n",
    "    member_data = getassignments(congress)\n",
    "    print(\"member_data success\")\n",
    "    hearing_data = gethearingdata(congress,member_data)\n",
    "    print(\"hearing_data success\")\n",
    "    match = testmatches(hearing_data)\n",
    "    print(\"match success\")\n",
    "    if len(match) > 0:\n",
    "        display([i for i in match[0]])\n",
    "        return(member_data,hearing_data)\n",
    "    else:\n",
    "        results = getconflicts(member_data,hearing_data)\n",
    "        return(results,member_data,hearing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Everything for Specific Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "members\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m congress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCongress (3-digit number): \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m results,member_data,hearing_data \u001b[38;5;241m=\u001b[39m \u001b[43mruneverything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcongress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m display(results)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# XXX COMMIT TO REPO\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# results.to_csv(str(\"C:\\\\Users\\\\rorey\\\\OneDrive - Bipartisan Policy Center\\\\Congress\\\\Modernization\\\\Scheduling Conflicts\\\\NEW\\\\Results\\\\\"+str(congress)+\"\\\\results\"+str(congress)+\".csv\"),index=False)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# hearing_data.to_csv(str(\"C:\\\\Users\\\\rorey\\\\OneDrive - Bipartisan Policy Center\\\\Congress\\\\Modernization\\\\Scheduling Conflicts\\\\NEW\\\\Results\\\\\"+str(congress)+\"\\\\hearings\"+str(congress)+\".csv\"),index=False)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# member_data.to_csv(str(\"C:\\\\Users\\\\rorey\\\\OneDrive - Bipartisan Policy Center\\\\Congress\\\\Modernization\\\\Scheduling Conflicts\\\\NEW\\\\Results\\\\\"+str(congress)+\"\\\\assignments\"+str(congress)+\".csv\"))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m, in \u001b[0;36mruneverything\u001b[0;34m(congress)\u001b[0m\n\u001b[1;32m      3\u001b[0m member_data \u001b[38;5;241m=\u001b[39m getassignments(congress)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmembers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m hearing_data \u001b[38;5;241m=\u001b[39m \u001b[43mgethearingdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcongress\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmember_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhearings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m match \u001b[38;5;241m=\u001b[39m testmatches(hearing_data)\n",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m, in \u001b[0;36mgethearingdata\u001b[0;34m(congress, member_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m hearing_data \u001b[38;5;241m=\u001b[39m gethearingrange(dates_start\u001b[38;5;241m.\u001b[39mget(congress),dates_end\u001b[38;5;241m.\u001b[39mget(congress))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#import replacement (comcode) files\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m replacement \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/rachelorey/Scheduling-Conflicts-in-Congress/master/replacement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcongress\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# #drop all comcodes without member assignments from clerk.gov\u001b[39;00m\n\u001b[1;32m     19\u001b[0m codes_to_drop \u001b[38;5;241m=\u001b[39m [value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m replacement[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m member_data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39munique()]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "congress = int(input(\"Congress (3-digit number): \"))\n",
    "\n",
    "results,member_data,hearing_data = runeverything(congress)\n",
    "\n",
    "display(results)\n",
    "# XXX COMMIT TO REPO\n",
    "\n",
    "\n",
    "# results.to_csv(str(congress)+\"results.csv\",index=False)\n",
    "# hearing_data.to_csv(str(congress)+\"hearings.csv\",index=False)\n",
    "# member_data.to_csv(str(congress)+\"assignments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PART ONE: GATHER COMMITTEE ASSIGNMENTS (this part takes forever if you're doing prior congresses)\n",
    "\n",
    "# ##PRE 2020\n",
    "# def getassignments(congress):\n",
    "#     import requests \n",
    "#     import pandas as pd\n",
    "#     from lxml import html    \n",
    "#     from datetime import datetime, timedelta\n",
    "    \n",
    "#     Com_Dict = {117:\"\",\n",
    "#                116:\"https://web.archive.org/web/20191219040247/\"}\n",
    "\n",
    "\n",
    "\n",
    "#     member_data = pd.DataFrame()\n",
    "\n",
    "#     clerk = Com_Dict.get(congress)+\"http://clerk.house.gov/committee_info/index.aspx\"\n",
    "#     webpage = requests.get(clerk)\n",
    "#     tree = html.fromstring(webpage.content)\n",
    "\n",
    "#     com_titles = tree.xpath(\"//div[@id='com_directory']//ul//li//a//text()\")\n",
    "#     com_links = tree.xpath(\"//div[@id='com_directory']//ul//li//a//@href\")\n",
    "\n",
    "#     ComLink_Dict = {117:\"http://clerk.house.gov\",\n",
    "#                116:\"https://web.archive.org/\"}\n",
    "\n",
    "#     com_links = [ComLink_Dict.get(congress)+end for end in com_links]\n",
    "#     com_codes = [title[title.find(\"=\")+1:len(title)] for title in com_links]\n",
    "\n",
    "#     for com in range(len(com_links)):\n",
    "#         singlecom = requests.get(com_links[com])\n",
    "#         tree = html.fromstring(singlecom.content)\n",
    "\n",
    "#         members = tree.xpath(\"//div[@id='primary_group' or @id='secondary_group']//ol//li/a/text()\")\n",
    "#         members = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in members]\n",
    "#         members = [str(member).replace(\"??\",\"e\") for member in members]\n",
    "\n",
    "#         member_data = member_data.append(pd.DataFrame(members,columns=[com_codes[com]]).transpose())\n",
    "\n",
    "#         subcom_links = tree.xpath(\"//div[@id='subcom_list']//ul//li//@href\")\n",
    "#         subcom_links = [ComLink_Dict.get(congress)+end for end in subcom_links]\n",
    "#         subcom_links = [link.replace(\"///\",\"/\") for link in subcom_links]\n",
    "\n",
    "#         subcom_codes = [title[title.find(\"=\")+1:len(title)] for title in subcom_links]\n",
    "\n",
    "#         for subcom in range(len(subcom_links)):\n",
    "#             single_subcom = requests.get(subcom_links[subcom])\n",
    "#             tree = html.fromstring(single_subcom.content)\n",
    "#             members = tree.xpath(\"//div[@id='primary_group' or @id='secondary_group']//ol//li/a/text()\")\n",
    "#             members = [sub.encode(\"ascii\", \"replace\").decode(\"utf-8\") for sub in members]\n",
    "#             members = [str(member).replace(\"??\",\"e\") for member in members]\n",
    "#             member_data = member_data.append(pd.DataFrame(members,columns=[subcom_codes[subcom]]).transpose())\n",
    "            \n",
    "#     return(member_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
